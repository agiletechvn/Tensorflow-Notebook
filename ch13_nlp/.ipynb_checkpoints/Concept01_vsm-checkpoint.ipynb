{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch `12`: Concept `01`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fabricate some data and store as `corpus_raw` lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = \"\"\"Deep Learning has evolved from Artificial Neural Networks, which has been\n",
    "there since the 1940s. Neural Networks are interconnected networks of processing units\n",
    "called artificial neurons that loosely mimic axons in a biological brain. In a biological\n",
    "neuron, the dendrites receive input signals from various neighboring neurons, typically\n",
    "greater than 1000. These modified signals are then passed on to the cell body or soma of\n",
    "the neuron, where these signals are summed together and then passed on to the axon of the\n",
    "neuron. If the received input signal is more than a specified threshold, the axon will\n",
    "release a signal which again will pass on to neighboring dendrites of other neurons. Figure\n",
    "2-1 depicts the structure of a biological neuron for reference. The artificial neuron units\n",
    "are inspired by the biological neurons with some modifications as per convenience. Much\n",
    "like the dendrites, the input connections to the neuron carry the attenuated or amplified\n",
    "input signals from other neighboring neurons. The signals are passed on to the neuron, where\n",
    "the input signals are summed up and then a decision is taken what to output based on the\n",
    "total input received. For instance, for a binary threshold neuron an output value of 1 is\n",
    "provided when the total input exceeds a pre-defined threshold; otherwise, the output stays\n",
    "at 0. Several other types of neurons are used in artificial neural networks, and their\n",
    "implementation only differs with respect to the activation function on the total input to\n",
    "produce the neuron output. In Figure 2-2 the different biological equivalents are tagged in\n",
    "the artificial neuron for easy analogy and interpretation.\"\"\"\n",
    "corpus_raw = corpus_raw.replace('\\n', ' ').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(ind, vocab_size):\n",
    "    rec = np.zeros(vocab_size)\n",
    "    rec[ind] = 1\n",
    "    return rec\n",
    "\n",
    "def create_training_data(corpus_raw, WINDOW_SIZE = 2):\n",
    "    words_list = []\n",
    "    sentences_list = corpus_raw.split('.')    \n",
    "    for sent in sentences_list:\n",
    "        for w in sent.split(' '):\n",
    "            if w != '.':\n",
    "                words_list.append(w.split('.')[0]) # remove if delimiter is tied to the end of a word\n",
    "                \n",
    "    words_list = set(words_list)  # remove the duplicates for each word\n",
    "    word2ind = {} # converting word to index\n",
    "    ind2word = {} # retrieving word from index\n",
    "    vocab_size = len(words_list)\n",
    "    for i,w in enumerate(words_list):\n",
    "        word2ind[w] = i\n",
    "        ind2word[i] = w\n",
    "    # print the result for testing\n",
    "    print(word2ind)\n",
    "    sentences = []\n",
    "    \n",
    "    for sent in sentences_list:\n",
    "        sent_array = sent.split()\n",
    "        sent_array = [s.split('.')[0] for s in sent_array]\n",
    "        sentences.append(sent_array)\n",
    "        \n",
    "    data_recs = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for ind,w in enumerate(sent):\n",
    "            rec = []\n",
    "            for nb_w in sent[max(ind - WINDOW_SIZE, 0) : min(ind + WINDOW_SIZE, len(sent)) + 1]:\n",
    "                if nb_w != w:\n",
    "                    rec.append(nb_w)\n",
    "                data_recs.append([rec, w])\n",
    "    x_train,y_train = [],[]\n",
    "    \n",
    "    for rec in data_recs:\n",
    "        input_ = np.zeros(vocab_size)\n",
    "        for i in range(WINDOW_SIZE - 1):\n",
    "            input_ += one_hot(word2ind[rec[0][i]], vocab_size)\n",
    "        input_ = input_ / len(rec[0])\n",
    "        x_train.append(input_)\n",
    "        y_train.append(one_hot(word2ind[rec[1]], vocab_size))\n",
    "    \n",
    "    return x_train, y_train, word2ind, ind2word, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining the model, let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'neural': 1, 'with': 2, 'neighboring': 3, 'than': 4, 'taken': 5, 'pass': 6, 'types': 7, 'threshold;': 8, 'analogy': 9, 'depicts': 10, 'again': 11, 'dendrites': 12, 'there': 13, 'output': 14, 'only': 15, 'body': 16, 'deep': 17, 'instance,': 18, 'that': 19, 'soma': 20, 'on': 21, 'or': 22, 'evolved': 23, 'learning': 24, 'been': 25, 'implementation': 26, 'these': 27, 'greater': 28, 'inspired': 29, 'function': 30, 'receive': 31, 'provided': 32, 'and': 33, '1': 34, 'used': 35, 'a': 36, 'of': 37, 'together': 38, 'artificial': 39, 'interconnected': 40, 'in': 41, 'to': 42, 'some': 43, 'neuron,': 44, 'dendrites,': 45, 'then': 46, 'passed': 47, 'decision': 48, 'an': 49, 'called': 50, 'are': 51, 'is': 52, 'the': 53, 'total': 54, 'stays': 55, 'where': 56, 'other': 57, 'since': 58, 'neurons,': 59, '1000': 60, 'will': 61, 'mimic': 62, 'processing': 63, 'more': 64, 'differs': 65, 'pre-defined': 66, 'up': 67, 'typically': 68, 'when': 69, 'axon': 70, 'biological': 71, 'easy': 72, 'per': 73, 'connections': 74, 'structure': 75, 'carry': 76, '1940s': 77, 'networks': 78, '2-1': 79, '0': 80, 'figure': 81, 'threshold': 82, 'modified': 83, 'respect': 84, 'interpretation': 85, 'amplified': 86, 'their': 87, 'if': 88, 'brain': 89, 'neurons': 90, 'specified': 91, 'summed': 92, 'modifications': 93, 'like': 94, 'tagged': 95, 'has': 96, 'networks,': 97, 'at': 98, 'by': 99, '2-2': 100, 'input': 101, 'as': 102, 'exceeds': 103, 'what': 104, 'several': 105, 'different': 106, 'reference': 107, 'equivalents': 108, 'cell': 109, 'which': 110, 'signal': 111, 'from': 112, 'for': 113, 'threshold,': 114, 'convenience': 115, 'loosely': 116, 'much': 117, 'various': 118, 'neuron': 119, 'axons': 120, 'value': 121, 'activation': 122, 'units': 123, 'based': 124, 'release': 125, 'binary': 126, 'attenuated': 127, 'otherwise,': 128, 'signals': 129, 'received': 130, 'produce': 131}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1fb760a92883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, y_train, word2ind, ind2word, vocab_size = create_training_data(corpus_raw, 2)\n",
    "emb_dims = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# placeholders for input and output\n",
    "x = tf.placeholder(tf.float32, [None, vocab_size])\n",
    "y = tf.placeholder(tf.float32, [None, vocab_size])\n",
    "\n",
    "# define embedding matrix weights and a bias\n",
    "W = tf.Variable(tf.random_normal([vocab_size, emb_dims], mean=0.0, stddev=0.02, dtype=tf.float32))\n",
    "b = tf.Variable(tf.random_normal([emb_dims], mean=0.0, stddev=0.02, dtype=tf.float32))\n",
    "W_outer = tf.Variable(tf.random_normal([emb_dims, vocab_size], mean=0.0, stddev=0.02, dtype=tf.float32))\n",
    "b_outer = tf.Variable(tf.random_normal([vocab_size], mean=0.0, stddev=0.02, dtype=tf.float32))\n",
    "\n",
    "hidden = tf.add(tf.matmul(x, W),b)\n",
    "logits = tf.add(tf.matmul(hidden, W_outer), b_outer)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "epochs, batch_size = 100, 10\n",
    "batch = len(x_train) # batch_size\n",
    "\n",
    "# train for n iterations\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        batch_index = 0\n",
    "        for batch_num in range(batch):\n",
    "            x_batch = x_train[batch_index: batch_index + batch_size]\n",
    "            y_batch = y_train[batch_index: batch_index + batch_size]\n",
    "            sess.run(optimizer, feed_dict={x:x_batch, y:y_batch})\n",
    "            if epoch % 10 == 0:\n",
    "                print('epoch:', epoch, 'loss:', sess.run(cost, feed_dict={x:x_batch, y:y_batch}))\n",
    "    \n",
    "    W_embed_trained = sess.run(W)\n",
    "    \n",
    "W_embedded = TSNE(n_components=2).fit_transform(W_embed_trained)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(len(W_embedded)):\n",
    "    plt.text(W_embedded[i,0], W_embedded[i,1], ind2word[i])\n",
    "    \n",
    "    \n",
    "plt.xlim(-150, 150)\n",
    "plt.ylim(-150, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
